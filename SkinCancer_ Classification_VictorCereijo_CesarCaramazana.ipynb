{"cells":[{"cell_type":"markdown","source":["Víctor Cereijo Herranz\n","\n","César Caramazana Zarzosa\n","\n","13/05/2023"],"metadata":{"id":"wON_GpzLk-ZW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6W6nMKM6bFw"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"Zpkh90E36bF0"},"source":["## Computer Vision\n","## Master in Information Health Engineering\n","\n","# LAB SESSION 6: IMAGE CLASSIFICATION WITH CNNs\n","\n","# AUTOMATIC DIAGNOSTIC SYSTEM OF SKIN LESSIONS FROM DERMOSCOPIC IMAGES\n","\n","\n","### Iván González Díaz\n","\n","\n","<center><img src='http://www.tsc.uc3m.es/~igonzalez/images/logo_uc3m_foot.jpg' width=400 /></center>"]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"YbkIRmwZpixh"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cqInIMw06bF4","executionInfo":{"status":"ok","timestamp":1684011032213,"user_tz":-120,"elapsed":11785,"user":{"displayName":"CESAR CARAMAZANA ZARZOSA","userId":"03627639155322478238"}},"outputId":"a5648285-49f2-4f3a-ce16-bc160be2e851"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["from __future__ import print_function, division\n","import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform, util\n","from sklearn import metrics\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils, models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import time\n","import copy\n","from PIL import Image\n","import pdb\n","import random\n","import numpy.random as npr\n","\n","random.seed(42)\n","npr.seed(42)\n","torch.manual_seed(42)\n","torch.backends.cudnn.enabled = False\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWr5FW9z6bF6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"43449676-b271-4cd6-d30a-3cd58ed68768","executionInfo":{"status":"ok","timestamp":1684011061681,"user_tz":-120,"elapsed":29480,"user":{"displayName":"CESAR CARAMAZANA ZARZOSA","userId":"03627639155322478238"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#ONLY TO USE GOOGLE COLAB. Run this code only the first time you run this notebook and then comment these lines\n","\n","from shutil import copyfile\n","from google.colab import drive\n","import os, sys\n","drive.mount('/content/drive', force_remount=True)\n","copyfile('/content/drive/My Drive/MASTER/Computer Vision/Lab6/db1.zip', './db1.zip') #Copy db files to our working folder\n","copyfile('/content/drive/My Drive/MASTER/Computer Vision/Lab6/db2.zip', './db2.zip')\n","\n","path_to_folder = '/content/drive/My Drive/MASTER/Computer Vision/Lab6'  \n","os.chdir(path_to_folder) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCUQW9oz6bF7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684011061681,"user_tz":-120,"elapsed":12,"user":{"displayName":"CESAR CARAMAZANA ZARZOSA","userId":"03627639155322478238"}},"outputId":"d497f716-78d7-4a38-aa7d-f430f6fbc76a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nimport zipfile\\nzipPath='./db1.zip' #path of the 1st zip file\\ndataFolder='./data' #We extract files to the current folder\\nwith zipfile.ZipFile(zipPath, 'r') as zip_ref:\\n    zip_ref.extractall(dataFolder)\\n    \\nzipPath='./db2.zip' #path of the 2nd zip file\\ndataFolder='./data' # We extract files to the current folder\\nwith zipfile.ZipFile(zipPath, 'r') as zip_ref:\\n    zip_ref.extractall(dataFolder)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["#NOTE: Run this only once, in the machine where you want to run your code, then comment these lines\n","\"\"\"\n","import zipfile\n","zipPath='./db1.zip' #path of the 1st zip file\n","dataFolder='./data' #We extract files to the current folder\n","with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n","    zip_ref.extractall(dataFolder)\n","    \n","zipPath='./db2.zip' #path of the 2nd zip file\n","dataFolder='./data' # We extract files to the current folder\n","with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n","    zip_ref.extractall(dataFolder)\n","\"\"\"    "]},{"cell_type":"markdown","metadata":{"id":"8yYPd4sx6bF9"},"source":["#Class Dataset\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBG4aYLN6bF9"},"outputs":[],"source":["class DermoscopyDataset(Dataset):\n","    \"\"\"Dermoscopy dataset.\"\"\"\n","\n","    def __init__(self, csv_file, root_dir,transform=None,  maxSize=0):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path al fichero csv con las anotaciones.\n","            root_dir (string): Directorio raíz donde encontraremos las carpetas 'images' y 'masks' .\n","            transform (callable, optional): Transformaciones opcionales a realizar sobre las imágenes.\n","        \"\"\"\n","        self.dataset = pd.read_csv(csv_file,header=0,dtype={'id': str, 'label': int})\n","        \n","        if maxSize>0:\n","            newDatasetSize=maxSize #maxSize muestras\n","            idx=np.random.RandomState(seed=42).permutation(range(len(self.dataset)))\n","            reduced_dataset=self.dataset.iloc[idx[0:newDatasetSize]]\n","            self.dataset=reduced_dataset.reset_index(drop=True)\n","\n","        self.root_dir = root_dir\n","        self.img_dir = os.path.join(root_dir,'images') \n","        self.mask_dir = os.path.join(root_dir,'masks')\n","        self.transform = transform\n","        self.classes = ['nevus', 'melanoma', 'keratosis']\n","\n","        \n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        #Leemos la imagen\n","        img_name = os.path.join(self.img_dir,self.dataset.id[idx] + '.jpg')\n","        image = io.imread(img_name)\n","        #Leemos la máscara\n","        mask_name = os.path.join(self.mask_dir,self.dataset.id[idx] + '.png')\n","        mask = io.imread(mask_name)\n","        \n","        sample = {'image': image, 'mask': mask, 'label':  self.dataset.label[idx].astype(dtype=np.long)}\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample"]},{"cell_type":"markdown","source":["# Transformations"],"metadata":{"id":"29OyH8JApTWb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eSU6Os96bF_"},"outputs":[],"source":["from skimage.util import random_noise\n","\n","class CropByMask(object):\n","    \"\"\"Crop the image using the lesion mask.\n","\n","    Args:\n","        border (tuple or int): Border surrounding the mask. We dilate the mask as the skin surrounding \n","        the lesion is important for dermatologists.\n","        If it is a tuple, then it is (bordery,borderx)\n","    \"\"\"\n","\n","    def __init__(self, border):\n","        assert isinstance(border, (int, tuple))\n","        if isinstance(border, int):\n","            self.border = (border,border)\n","        else:\n","            self.border = border\n","            \n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        h, w = image.shape[:2]\n","        #Compute the coordinates of the bounding box that contains the mask \n","        sidx=np.nonzero(mask)\n","        minx=np.maximum(sidx[1].min()-self.border[1],0)\n","        maxx=np.minimum(sidx[1].max()+1+self.border[1],w)\n","        miny=np.maximum(sidx[0].min()-self.border[0],0)\n","        maxy=np.minimum(sidx[0].max()+1+self.border[1],h)\n","        #Crop the image\n","        image=image[miny:maxy,minx:maxx,...]\n","        mask=mask[miny:maxy,minx:maxx]\n","\n","        return {'image': image, 'mask': mask, 'label' : label}\n","    \n","class Rescale(object):\n","    \"\"\"Re-scale image to a predefined size.\n","\n","    Args:\n","        output_size (tuple or int): The desired size. If it is a tuple, output is the output_size. \n","        If it is an int, the smallest dimension will be the output_size\n","            a we will keep fixed the original aspect ratio.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        h, w = image.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","\n","        new_h, new_w = int(new_h), int(new_w)\n","\n","        img = transform.resize(image, (new_h, new_w))\n","        msk = transform.resize(mask, (new_h, new_w))\n","\n","        return {'image': img, 'mask': msk, 'label' : label}\n","\n","class RandomCrop(object):\n","    \"\"\"Randomly crop the image.\n","\n","    Args:\n","        output_size (tuple or int): Crop size. If  int, square crop\n","\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","\n","        if h>new_h:\n","            top = np.random.randint(0, h - new_h)\n","        else:\n","            top=0\n","            \n","        if w>new_w: \n","            left = np.random.randint(0, w - new_w)\n","        else:\n","            left = 0\n","            \n","        image = image[top: top + new_h,\n","                     left: left + new_w]\n","\n","        mask = mask[top: top + new_h,\n","                      left: left + new_w]\n","\n","\n","        return {'image': image, 'mask': mask, 'label': label}\n","    \n","class CenterCrop(object):\n","    \"\"\"Crop the central area of the image\n","\n","    Args:\n","        output_size (tupla or int): Crop size. If int, square crop\n","\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","        rem_h = h - new_h\n","        rem_w = w - new_w\n","        \n","        if h>new_h:\n","            top = int(rem_h/2)\n","        else:\n","            top=0\n","            \n","        if w>new_w: \n","            left = int(rem_w/2)\n","        else:\n","            left = 0\n","            \n","        image = image[top: top + new_h,\n","                     left: left + new_w]\n","\n","        mask = mask[top: top + new_h,\n","                      left: left + new_w]\n","\n","\n","        return {'image': image, 'mask': mask, 'label': label}\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert ndarrays into pytorch tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        # Cambiamos los ejes\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.transpose((2, 0, 1))\n","        image = torch.from_numpy(image)\n","        # A la máscara le añadimos una dim fake al principio\n","        mask = torch.from_numpy(mask)\n","        mask = mask.unsqueeze(0)\n","        label=torch.tensor(label,dtype=torch.long)\n","        \n","        return {'image':image,\n","                'mask':mask,\n","                'label':label}\n","    \n","class Normalize(object):\n","    \"\"\"Normalize data by subtracting means and dividing by standard deviations.\n","\n","    Args:\n","        mean_vec: Vector with means. \n","        std_vec: Vector with standard deviations.\n","    \"\"\"\n","\n","    def __init__(self, mean,std):\n","      \n","        assert len(mean)==len(std),'Length of mean and std vectors is not the same'\n","        self.mean = np.array(mean)\n","        self.std = np.array(std)\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        c, h, w = image.shape\n","        assert c==len(self.mean), 'Length of mean and image is not the same' \n","        dtype = image.dtype\n","        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n","        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n","        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n","    \n","        \n","        return {'image': image, 'mask': mask, 'label' : label}\n","\n","class TVCenterCrop(object):\n","    \"\"\"Crop the central area of the image. Example using the method in torchvision. Requires to\n","    internally convert from skimage (numpy array) to PIL Image\n","\n","    Args:\n","        output_size (tupla or int): Crop size. If int, square crop\n","\n","    \"\"\"\n","\n","    def __init__(self, size):\n","        self.CC=transforms.CenterCrop(size)\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        pil_image=Image.fromarray(util.img_as_ubyte(image))\n","        pil_image=self.CC(pil_image)\n","        image=util.img_as_float(np.asarray(pil_image))\n","        \n","        pil_mask=Image.fromarray(util.img_as_ubyte(mask))\n","        pil_mask=self.CC(pil_mask)\n","        mask=util.img_as_float(np.asarray(pil_mask))\n","        \n","        return {'image': image, 'mask': mask, 'label': label}        \n","\n","### Put your code here....\n","\n","class HorizontalFlip(object):\n","  \n","  def __init__(self, p=0.5):\n","    self.hf = transforms.RandomHorizontalFlip(p=p)\n","  \n","  def __call__(self, sample):\n","    image, mask, label = sample['image'], sample['mask'], sample['label']\n","\n","    #Set seed to apply the same transformation to img and mask\n","    seed = np.random.randint(34193483)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    pil_image=Image.fromarray(util.img_as_ubyte(image))\n","    pil_image=self.hf(pil_image)\n","    image=util.img_as_float(np.asarray(pil_image))\n","    \n","    pil_mask=Image.fromarray(util.img_as_ubyte(mask))\n","    pil_mask=self.hf(pil_mask)\n","    mask=util.img_as_float(np.asarray(pil_mask))\n","\n","    return {'image': image, 'mask': mask, 'label': label}\n","\n","\n","class VerticalFlip(object):\n","  \n","  def __init__(self, p=0.5):\n","    self.vf = transforms.RandomVerticalFlip(p=p)\n","  \n","  def __call__(self, sample):\n","    image, mask, label = sample['image'], sample['mask'], sample['label']\n","\n","    seed = np.random.randint(34193483)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    pil_image=Image.fromarray(util.img_as_ubyte(image))\n","    pil_image=self.vf(pil_image)\n","    image=util.img_as_float(np.asarray(pil_image))\n","    \n","    pil_mask=Image.fromarray(util.img_as_ubyte(mask))\n","    pil_mask=self.vf(pil_mask)\n","    mask=util.img_as_float(np.asarray(pil_mask))\n","\n","    return {'image': image, 'mask': mask, 'label': label}\n","\n","\n","class Jitter(object):\n","  \n","  def __init__(self, brightness = 0.03, contrast=0.05, saturation=0.03, hue=0.03):\n","    \n","    self.jitter = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n","  \n","  def __call__(self, sample):\n","    image, mask, label = sample['image'], sample['mask'], sample['label']\n","\n","    pil_image=Image.fromarray(util.img_as_ubyte(image))\n","    pil_image=self.jitter(pil_image)\n","    image=util.img_as_float(np.asarray(pil_image))\n","\n","\n","    return {'image': image, 'mask': mask, 'label': label}\n","\n","class Rotation(object):\n","  def __init__(self, degrees=30):\n","    self.rotate = transforms.RandomRotation(degrees)\n","\n","  def __call__(self, sample):\n","    image, mask, label = sample['image'], sample['mask'], sample['label']\n","\n","    seed = np.random.randint(34193483)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    pil_image=Image.fromarray(util.img_as_ubyte(image))\n","    pil_image=self.rotate(pil_image)\n","    image=util.img_as_float(np.asarray(pil_image))\n","    \n","    pil_mask=Image.fromarray(util.img_as_ubyte(mask))\n","    pil_mask=self.rotate(pil_mask)\n","    mask=util.img_as_float(np.asarray(pil_mask))\n","\n","    return {'image': image, 'mask': mask, 'label': label}\n","\n","\n","class GaussianBlur(object):  \n","  def __init__(self, kernel=5, sigma=1):\n","    self.blur = transforms.GaussianBlur(kernel_size=kernel, sigma=(0.1, sigma))\n","  \n","  def __call__(self, sample):\n","    image, mask, label = sample['image'], sample['mask'], sample['label']\n","\n","    #Set seed to apply the same transformation to img and mask\n","    seed = np.random.randint(34193483)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    pil_image=Image.fromarray(util.img_as_ubyte(image))\n","    pil_image=self.blur(pil_image)\n","    image=util.img_as_float(np.asarray(pil_image))\n","    \n","    return {'image': image, 'mask': mask, 'label': label}\n","\n","class GaussianNoise(object):\n","  def __init__(self, sigma=1):\n","    self.sigma = sigma\n","    self.mean = 0\n","\n","  def __call__(self, sample):\n","    image, mask, label = sample['image'], sample['mask'], sample['label']  \n","\n","    seed = np.random.randint(34193483)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    gauss = np.random.normal(self.mean,self.sigma,image.shape)\n","    image = image + gauss\n","\n","    return {'image': image, 'mask': mask, 'label': label}\n","\n","    \n","        "]},{"cell_type":"markdown","source":["# The dataset"],"metadata":{"id":"nNhL9miKocIi"}},{"cell_type":"markdown","source":["## Data augmentation"],"metadata":{"id":"M663hLymi4LR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgkEMAZ26bGC"},"outputs":[],"source":["#Pixel means and stds expected by models in torchvision\n","pixel_mean=[0.485, 0.456, 0.406]\n","pixel_std=[0.229, 0.224, 0.225]\n","\n","rescale = 256\n","crop = 224\n","\n","#Train Dataset\n","train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv',\n","                                    root_dir='data',\n","                                    maxSize=2000, ###IMPORTANT: maxSize=500 to speed-up the training process\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(rescale),\n","                                    #Rotation(90),\n","                                    RandomCrop(crop),\n","                                    #Jitter(brightness=0.05, contrast=0.1, hue=0, saturation=0),\n","                                    HorizontalFlip(p=0.5),\n","                                    VerticalFlip(p=0.5), \n","                                    ToTensor(),\n","                                    Normalize(mean=pixel_mean,\n","                                    std=pixel_std)\n","                                    ]))\n","#Val dataset\n","val_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBval.csv',\n","                                    root_dir='data',\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(rescale),\n","                                    CenterCrop(crop),\n","                                    ToTensor(),\n","                                    Normalize(mean=pixel_mean,\n","                                    std=pixel_std)\n","                                    ]))\n","\n","#Test dataset\n","test_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtest.csv',\n","                                    root_dir='data',\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(rescale),\n","                                    CenterCrop(crop),\n","                                    ToTensor(),\n","                                    Normalize(mean=pixel_mean,\n","                                    std=pixel_std)\n","                                    ]))\n","    "]},{"cell_type":"markdown","source":["## Dataloaders"],"metadata":{"id":"YTL7MkH8i6UF"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"_KY-wtMS6bGC"},"outputs":[],"source":["#Specify training dataset, with a batch size of 8, shuffle the samples, and parallelize with 3 workers\n","train_dataloader = DataLoader(train_dataset, batch_size=128,\n","                        shuffle=True, num_workers=3)\n","#Validation dataset => No shuffle\n","val_dataloader = DataLoader(val_dataset, batch_size=32,\n","                        shuffle=False, num_workers=2)\n","\n","#Test Dataset => => No shuffle\n","test_dataloader = DataLoader(test_dataset, batch_size=32,\n","                        shuffle=False, num_workers=3)\n"]},{"cell_type":"code","source":["image_datasets = {'train' : train_dataset, 'val': val_dataset}\n","\n","dataloaders = {'train' : train_dataloader, 'val': val_dataloader}\n","          \n","dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n","class_names = image_datasets['train'].classes\n"],"metadata":{"id":"y_bqnQHzq1cK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Auxiliary functions"],"metadata":{"id":"35SSl8TLqVzr"}},{"cell_type":"markdown","source":["## Compute AUCs"],"metadata":{"id":"AgtIVfq9kyU5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"11bNQQss6bGE"},"outputs":[],"source":["#Function that computes 2 AUCs: melanoma vs all and keratosis vs all\n","# scores is nx3: n is the number of samples in the dataset \n","# labels is nx1\n","# Function resturns an array with two elements: the auc values\n","def computeAUCs(scores,labels):\n","                \n","    aucs = np.zeros((2,))\n","    #Calculamos el AUC melanoma vs all\n","    scores_mel = scores[:,1]\n","    labels_mel = (labels == 1).astype(np.int) \n","    aucs[0]=metrics.roc_auc_score(labels_mel, scores_mel)\n","\n","    #Calculamos el AUC queratosis vs all\n","    scores_sk = scores[:,2]\n","    labels_sk = (labels == 2).astype(np.int) \n","    aucs[1]=metrics.roc_auc_score(labels_sk, scores_sk)\n","    \n","    return aucs"]},{"cell_type":"markdown","source":["## Train model"],"metadata":{"id":"r7VwKABjkwIa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYzy1R-U6bGE"},"outputs":[],"source":["#train_model parameters are the network (model), the criterion (loss),\n","# the optimizer, a learning scheduler (una estrategia de lr strategy), and the training epochs\n","def train_model(model, criterion, optimizer, scheduler, num_epochs=25, output_dir='./Checkpoints/', model_name='custom'):\n","    since = time.time()\n","    \n","    numClasses = len(image_datasets['train'].classes)\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_aucs = np.zeros((2,)) #AUCs melanoma vs all, and keratosis\n","    best_auc = 0\n","    best_epoch = -1\n","    \n","    #Loop of epochs (each iteration involves train and val datasets)\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        \n","        \n","        # Cada época tiene entrenamiento y validación\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set the model in training mode\n","            else:\n","                model.eval()   # Set the model in val mode (no grads)\n","            \n","            #Dataset size\n","            numSamples = dataset_sizes[phase]\n","            \n","            # Create variables to store outputs and labels\n","            outputs_m=np.zeros((numSamples,numClasses),dtype=np.float)\n","            labels_m=np.zeros((numSamples,),dtype=np.int)\n","            running_loss = 0.0\n","            \n","            contSamples=0\n","            \n","            # Iterate (loop of batches)\n","            for sample in dataloaders[phase]:\n","                inputs = sample['image'].to(device).float()\n","                labels = sample['label'].to(device)                \n","                \n","                #Batch Size\n","                batchSize = labels.shape[0]\n","                \n","                # Set grads to zero\n","                optimizer.zero_grad()\n","\n","                # Forward\n","                # Register ops only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward & parameters update only in train\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","                \n","                # Accumulate the running loss\n","                running_loss += loss.item() * inputs.size(0)\n","                \n","                #Apply a softmax to the output\n","                outputs=F.softmax(outputs.data,dim=1)\n","                # Store outputs and labels \n","                outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n","                labels_m [contSamples:contSamples+batchSize]=labels.cpu().numpy()\n","                contSamples+=batchSize\n","                \n","            #At the end of an epoch, update the lr scheduler    \n","            if phase == 'train':\n","                scheduler.step()\n","            \n","            #Accumulated loss by epoch\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            \n","            #Compute the AUCs at the end of the epoch\n","            aucs=computeAUCs(outputs_m,labels_m)\n","            \n","            #And the Average AUC\n","            epoch_auc = aucs.mean()\n","                         \n","            print('{} Loss: {:.4f} AUC mel: {:.4f} sk: {:.4f} avg: {:.4f}'.format(\n","                phase, epoch_loss, aucs[0], aucs[1], epoch_auc))\n","\n","            #Save checkpoint\n","            path = output_dir + str(model_name) + str(epoch) +'.pt'\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'epoch': epoch,\n","                }, os.path.join(path))\n","            \n","            # Deep copy of the best model\n","            if phase == 'val' and epoch_auc > best_auc:\n","                best_auc = epoch_auc\n","                best_aucs = aucs.copy()        \n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                best_epoch = epoch\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best model in epoch {:d} val AUCs: mel {:4f} sk {:4f} avg {:4f}'.format(best_epoch,best_aucs[0],best_aucs[1],best_auc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"]},{"cell_type":"markdown","source":["## Test model"],"metadata":{"id":"eRMEgQwrksur"}},{"cell_type":"code","source":["### Code that generates the test matrix\n","def test_model(model):\n","    since = time.time()\n","    \n","    numClasses = len(test_dataset.classes)\n","    \n","    model.eval()   # Ponemos el modelo en modo evaluación\n","\n","    #Tamaño del dataset\n","    numSamples = len(test_dataset)\n","            \n","    # Creamos las variables que almacenarán las salidas y las etiquetas\n","    outputs_m=np.zeros((numSamples,numClasses),dtype=np.float)\n","    labels_m=np.zeros((numSamples,),dtype=np.int)\n","    contSamples=0\n","            \n","    # Iteramos sobre los datos\n","    for sample in test_dataloader:\n","        inputs = sample['image'].to(device).float()\n","                \n","                \n","        #Tamaño del batch\n","        batchSize = inputs.shape[0]\n","                \n","        # Paso forward\n","        with torch.torch.no_grad():\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","                \n","            #Aplicamos un softmax a la salida\n","            outputs=F.softmax(outputs.data,dim=1)\n","            outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n","            contSamples+=batchSize\n","                \n","            \n","    return outputs_m"],"metadata":{"id":"y_zHca0Ekrwv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMu5imz46bGC"},"source":["# PART 1) CUSTOM NETWORK\n","\n"]},{"cell_type":"markdown","source":["## Model architecture"],"metadata":{"id":"bCVy_fQhjHJt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6vRS1SD6bGD"},"outputs":[],"source":["#Example network ModifiedCustmoNet\n","class CustomNet(nn.Module):\n","    def __init__(self):\n","        super(CustomNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(128)\n","        self.conv3 = nn.Conv2d(128, 212, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(212)\n","        self.conv4 = nn.Conv2d(212, 256, kernel_size=3, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.fc1 = nn.Linear(256 * 14*14, 512)\n","        self.fc2 = nn.Linear(512, 3)\n","        self.dropout = nn.Dropout(0.5)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = F.max_pool2d(x, 2)\n","        x = x.flatten(1)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"FaY5vx8hqgEE"}},{"cell_type":"code","source":["def weights_init_custom(m):\n","    if isinstance(m, torch.nn.Conv2d):\n","      torch.nn.init.xavier_normal_(m.weight,1.0)\n","      if m.bias is not None:\n","          nn.init.constant_(m.bias.data, 0)\n","          \n","    elif isinstance(m, torch.nn.Linear):\n","      torch.nn.init.xavier_normal_(m.weight,1.0)\n","      if m.bias is not None:\n","          nn.init.constant_(m.bias.data, 0)   \n","\n","\n","customNet = CustomNet() #we initialize the network\n","customNet.to(device) #copy the network to the device (gpu)\n","load = False\n","\n","if load:\n","  path = './Checkpoints/custom_another18.pt'\n","  customNet.load_state_dict(torch.load(path)['model_state_dict'])\n","  customNet.to(device)\n","else:  \n","  #Weight init\n","  customNet.apply(weights_init_custom)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# SGD with momentum \n","#optimizer_ft = optim.SGD(customNet.parameters(), lr=1e-3, momentum=0.9)\n","optimizer_ft = optim.Adam(customNet.parameters(), lr=1e-3, weight_decay=1e-3) #optimizer\n","\n","\n","\n","# An lr strategy which decreases lr by a factor of 0.1 every 7 epochs \n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"metadata":{"id":"eKlDjDktFQMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNQnu8NJ6bGF","scrolled":false},"outputs":[],"source":["#we will train the model for 30 epochs\n","random.seed(42)\n","npr.seed(42)\n","torch.manual_seed(42)\n","\n","customNet = train_model(customNet, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=30, output_dir= './Checkpoints/', model_name='custom_another')"]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"CftZWF5YmfKe"}},{"cell_type":"code","source":["import csv     \n","\n","outputs=test_model(customNet)\n","\n","with open('output_test_custom.csv', mode='w') as out_file:\n","    csv_writer = csv.writer(out_file, delimiter=',')\n","    csv_writer.writerows(outputs);"],"metadata":{"id":"SfvmuKIoKa0W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YhyuPrQp6bGG"},"source":["# PART 2) PRETRAINED NETWORK\n","\n"]},{"cell_type":"markdown","source":["## Model architecture"],"metadata":{"id":"c1f_7UzDjrTa"}},{"cell_type":"code","source":["def weights_init(m):\n","    if isinstance(m, torch.nn.Linear):\n","      torch.nn.init.xavier_normal_(m.weight,1.0)\n","      if m.bias is not None:\n","          nn.init.constant_(m.bias.data, 0)      "],"metadata":{"id":"kq1TJbz0qpgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RESNET\n","model_ft = models.resnet101(pretrained=True)\n","\n","num_classes = len(train_dataset.classes)\n","model_ft.fc = nn.Sequential(\n","    nn.Dropout(0.3),\n","    nn.Linear(2048, num_classes)\n",")\n","\n","\n","#Resume training\n","load = False\n","if load:\n","  path = './Checkpoints/final_pretrained.pt'\n","  model_ft.load_state_dict(torch.load(path)['model_state_dict'])\n","else:  \n","  #Weight init\n","  model_ft.apply(weights_init)\n","\n","model_ft.to(device)"],"metadata":{"id":"IBqgeduOjs03"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"CnZuUKIrjttK"}},{"cell_type":"code","source":["#Freeze layers\n","for p in model_ft.parameters():\n","  p.requires_grad = False\n","\n","for p in model_ft.layer4.parameters():\n","  p.requires_grad = True\n","\n","#Except classifier  \n","for p in model_ft.fc.parameters():\n","  p.requires_grad = True  \n","\n","p1 = [p for p in model_ft.layer4.parameters() if p.requires_grad]\n","p2 = [p for p in model_ft.fc.parameters() if p.requires_grad]\n","\n","class_weights = [1/0.686, 1/0.187, 1/0.127] / np.sum([1/0.686, 1/0.187, 1/0.127])\n","class_weights = torch.from_numpy(class_weights).float().to(device)\n","\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\n","\n","\n","optimizer_ft = optim.Adam([{'params': p1, 'lr': 1e-4}, \n","                        {'params': p2, 'lr': 1e-3},\n","                        ], lr=1e-3, weight_decay=1e-2)\n","\n","\n","\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)"],"metadata":{"id":"WVYPfrF6Kd8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCd_bc986bGH"},"outputs":[],"source":["#We fix seeds for reproducibility\n","random.seed(42)\n","npr.seed(42)\n","torch.manual_seed(42)\n","\n","model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=25, model_name='pre_new')"]},{"cell_type":"markdown","metadata":{"id":"Pc0CUnRy6bGH"},"source":["## Test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDkhtRMP6bGI"},"outputs":[],"source":["import csv\n","\n","outputs=test_model(model_ft)\n","\n","with open('output_test_pretrained.csv', mode='w') as out_file:\n","    csv_writer = csv.writer(out_file, delimiter=',')\n","    csv_writer.writerows(outputs);"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[],"collapsed_sections":["YbkIRmwZpixh","8yYPd4sx6bF9","29OyH8JApTWb","eRMEgQwrksur"],"gpuType":"T4","toc_visible":true},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}